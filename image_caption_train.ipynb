{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Data generator\n",
    "    a. Loads vocab\n",
    "    b. Loads image features\n",
    "    c. provide data for training.\n",
    "2. Build image caption model.\n",
    "3. Trains the model.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "\n",
    "input_description_file = './data/results_20130124.token'\n",
    "input_img_feature_dir = './data/feature_extraction_inception_v3/'\n",
    "input_vocab_file = './data/vocab.txt'\n",
    "output_dir = './data/local_run'\n",
    "\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "    \n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold = 3,\n",
    "        num_embedding_nodes = 32,\n",
    "        num_timesteps = 35,\n",
    "        num_lstm_nodes = [64, 64],\n",
    "        num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 80,\n",
    "        cell_type = \"lstm\", # 控制用什么rnn的网络结构\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        keep_prob = 0.8,\n",
    "        log_frequent = 100,\n",
    "        save_frequent = 1000,\n",
    "    )\n",
    "\n",
    "hps = get_default_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 10875\n",
      "[1503, 389, 1, 0, 2]\n",
      "'the of man white'\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._id_to_word = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            # 如果这个词有重复的出现就抛出异常\n",
    "            if word in self._word_to_id or idx in self._id_to_word:\n",
    "                raise Exception(\"duplicate words in vocab.\")\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "            \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "        \n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "        \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "        \n",
    "    def id_to_word(self, idx):\n",
    "        return self._id_to_word.get(idx, '<UNK>')\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "        \n",
    "    def encode(self, sentence):\n",
    "        return [self.word_to_id(word) for word in sentence.split()]\n",
    "        \n",
    "    def decode(self, sentence_id):\n",
    "        return ' '.join([self.id_to_word(word_id) for word_id in sentence_id])\n",
    "        \n",
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "\n",
    "\n",
    "pprint.pprint(vocab.encode(\"I have a dream .\"))\n",
    "pprint.pprint(vocab.decode([5, 10, 9, 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num of all imgs: 31783\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n",
      "INFO:tensorflow:num of all imgs: 31783\n",
      "[[3, 9, 4, 132, 8, 3537, 6, 1, 48, 337, 146, 139, 1, 244, 93, 7, 380, 36, 2],\n",
      " [3, 20, 179, 11, 1, 26, 284, 7, 120, 128, 297, 6, 1, 93, 146, 2],\n",
      " [3, 9, 4, 1, 26, 21, 7, 120, 128, 8, 340, 6, 1, 93, 146, 2],\n",
      " [3, 63, 8, 340, 6, 1, 146, 12, 70, 15, 518, 2],\n",
      " [3, 9, 340, 6, 1, 146, 4, 1, 112, 171, 2]]\n"
     ]
    }
   ],
   "source": [
    "def parse_token_file(token_file):\n",
    "    \"\"\"Parses image description file\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    \"\"\"Converts tokens of each deacription of imgs to id.\"\"\"\n",
    "    img_name_to_tokens_id = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_tokens_id.setdefault(img_name, [])\n",
    "        for description in img_name_to_tokens[img_name]:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_tokens_id[img_name].append(token_ids)\n",
    "    return img_name_to_tokens_id\n",
    "\n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_tokens_id = convert_token_to_id(img_name_to_tokens, vocab)\n",
    "\n",
    "logging.info('num of all imgs: %d' % len(img_name_to_tokens))\n",
    "pprint.pprint(img_name_to_tokens['2778832101.jpg'])\n",
    "logging.info('num of all imgs: %d' % len(img_name_to_tokens_id))\n",
    "pprint.pprint(img_name_to_tokens_id['2778832101.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/feature_extraction_inception_v3/image_features-15.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-26.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-21.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-23.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-0.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-5.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-2.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-6.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-29.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-22.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-13.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-7.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-27.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-14.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-1.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-9.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-16.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-4.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-11.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-17.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-24.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-18.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-20.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-8.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-25.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-10.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-28.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-19.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-3.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-12.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-31.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-30.pickle']\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-15.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-26.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-21.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-23.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-0.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-5.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-2.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-6.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-29.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-22.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-13.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-7.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-27.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-14.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-1.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-9.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-16.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-4.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-11.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-17.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-24.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-18.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-20.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-8.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-25.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-10.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-28.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-19.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-3.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-12.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-31.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-30.pickle\n",
      "(31783, 2048)\n",
      "(31783,)\n",
      "INFO:tensorflow:img_feature_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 31783\n",
      "array([[0.33692122, 0.13584161, 1.1831442 , ..., 0.6633202 , 0.49345088,\n",
      "        0.7105526 ],\n",
      "       [0.21073194, 0.19380777, 0.04524017, ..., 0.91881   , 0.864785  ,\n",
      "        0.40993294],\n",
      "       [0.37982792, 0.09503828, 0.30234116, ..., 0.04770714, 0.17390908,\n",
      "        0.03280548],\n",
      "       [0.14858077, 0.21289283, 0.2699362 , ..., 0.52724296, 0.1713827 ,\n",
      "        0.39124614],\n",
      "       [0.36642122, 0.5907133 , 0.33421087, ..., 0.44528115, 0.2965037 ,\n",
      "        0.168856  ]], dtype=float32)\n",
      "array([[  16,   98,   14,   41,  254,    1, 1525,  101,    8,  899,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2],\n",
      "       [   3,   60,   30,   11,   22,  113,  589,    1, 1754,  210,    1,\n",
      "        7036,    7,    1, 1922,  311,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2],\n",
      "       [   3,    9,  734,  116,   25,  948,  805,   24,  264,   11,    1,\n",
      "          30,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2],\n",
      "       [   3,   23,    9,    4,    1,   61,   79,  906,    1, 1007,  148,\n",
      "         299,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2],\n",
      "       [   3,    9,    4,    1,   22,   21,    7,   26,  132,   87,    4,\n",
      "          38,   10,    1,  369,  235,   70,   15,    1,  301,    0,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "array(['4558067487.jpg', '6775386632.jpg', '279550225.jpg',\n",
      "       '345412253.jpg', '1916832697.jpg'], dtype='<U14')\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionData:\n",
    "    \"\"\"Provides data for image caption model\"\"\"\n",
    "    def __init__(self,\n",
    "                 img_name_to_tokens_id,\n",
    "                 img_feature_dir,\n",
    "                 num_timesteps,\n",
    "                 vocab,\n",
    "                 deterministic = False):   # 文本是否可以shuffle，False为可以\n",
    "        self._vocab = vocab\n",
    "        self._img_name_to_tokens_id = img_name_to_tokens_id\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._deterministic = deterministic\n",
    "        self._indicator = 0\n",
    "        \n",
    "        self._img_feature_filenames = [] # 存储图片名字\n",
    "        self._img_feature_data = [] # 存储图片的向量\n",
    "        \n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(\n",
    "                os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "        self._load_img_feature_pickle()\n",
    "        \n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "            \n",
    "    def _load_img_feature_pickle(self):\n",
    "        \"\"\"Load img feature data from pickle\"\"\"\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info(\"loading %s\" % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = pickle.load(f, encoding='latin1')\n",
    "                self._img_feature_filenames += filenames\n",
    "                self._img_feature_data.append(features)\n",
    "            # [#(1000, 1, 1, 2048), #(1000, 1, 1, 2048)] -> #(2000, 1, 1, 2048)\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        self._img_feature_data = np.reshape(\n",
    "            self._img_feature_data,\n",
    "            (origin_shape[0], origin_shape[3]))\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        self._img_feature_data = np.asarray(self._img_feature_data)\n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "            \n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "    \n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "            \n",
    "    def _random_shuffle(self):\n",
    "        \"\"\"Shuffle data randomly.\"\"\"\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "        \n",
    "    def _img_desc(self, batch_filenames):\n",
    "        \"\"\"Gets descriptions for filenames in batch.\"\"\"\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = [] # 避免不必要的计算\n",
    "        for filename in batch_filenames:\n",
    "            token_ids_set = self._img_name_to_tokens_id[filename]\n",
    "            chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids_length = len(chosen_token_ids)\n",
    "            \n",
    "            weight = [1 for i in range(chosen_token_ids_length)]\n",
    "            if chosen_token_ids_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_ids_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "        \n",
    "            \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return next batch data.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator < self.size()\n",
    "        \n",
    "        batch_filenames = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        # batch_weights的作用：ssentence_ids: [100, 101, 102, 10, 3, 0, 0, 0] -> [1, 1, 1, 1, 1, 0, 0, 0]\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_filenames)\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_filenames\n",
    "            \n",
    "caption_data = ImageCaptionData(img_name_to_tokens_id,\n",
    "                                input_img_feature_dir,\n",
    "                                hps.num_timesteps,\n",
    "                                vocab)\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info('img_feature_dim: %d' % img_feature_dim)\n",
    "logging.info('caption_data_size: %d' % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_filenames = caption_data.next_batch(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-90cc43c9c5fd>:54: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-5-90cc43c9c5fd>:5: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: img_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: img_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/logits/kernel:0\n",
      "INFO:tensorflow:variable name: fc/logits/bias:0\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    \"\"\"Returns specific cell according to cell_type.\"\"\"\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim,\n",
    "                                            state_is_tuple = True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s type has not been supported.\" % cell_type)\n",
    "        \n",
    "def dropout(cell, keep_prob):\n",
    "    \"\"\"Wrap cell with dropout.\"\"\"\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                         output_keep_prob = keep_prob)\n",
    "\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    img_feature = tf.placeholder(tf.float32,\n",
    "                                 (batch_size, img_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32,\n",
    "                              (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.int32,\n",
    "                          (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int32),\n",
    "                              name = 'global_step',\n",
    "                              trainable = False)\n",
    "    \"\"\" prediction process:\n",
    "            sentence: [a, b, c, d, e]\n",
    "            input: [img, a, b, c, d]\n",
    "            img_feature: [3, 4, 10, 2]\n",
    "            predict #1: img_feature -> embedding_img -> lstm -> (a)\n",
    "            predict #2: a -> embedding_word -> lstm -> (b)\n",
    "            predict #3: b -> embedding_word -> lstm -> (c)\n",
    "            ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sets up embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding',\n",
    "                           initializer = embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        # embed_token_ids: [batch_size, num_timesteps-1, num_embedding_nodes]\n",
    "        embed_token_ids = tf.nn.embedding_lookup(\n",
    "            embeddings,\n",
    "            sentence[:, 0: num_timesteps -1])\n",
    "        \n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(\n",
    "        factor = 1.0)\n",
    "    with tf.variable_scope('img_feature_embed',\n",
    "                           initializer = img_feature_embed_init):\n",
    "        # img_feature: [batch_size, img_feature_dim]\n",
    "        # embed_img: [batch_size, num_embedding_nodes]\n",
    "        embed_img = tf.layers.dense(img_feature,\n",
    "                                    hps.num_embedding_nodes)\n",
    "        # embed_img: [batch_size, 1, num_embedding_nodes]\n",
    "        embed_img = tf.expand_dims(embed_img, axis=1)\n",
    "        # embed_inputs: [batch_size, num_timesteps, num_embedding_nodes]\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis = 1)\n",
    "        \n",
    "    # Sets up rnn network\n",
    "    scale = 1.0 /math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    rnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer = rnn_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        \n",
    "        init_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # rnn_outputs: [batch_size, num_timestep, hps.num_lstm_node[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                            embed_inputs,\n",
    "                                            initial_state = init_state)\n",
    "    # Sets up fully_connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor = 1.0)\n",
    "    with tf.variable_scope('fc', initializer = fc_init):\n",
    "        rnn_outputs_2d = tf.reshape(rnn_outputs,\n",
    "                                    [-1, hps.num_lstm_nodes[-1]])\n",
    "        fc1 = tf.layers.dense(rnn_outputs_2d, hps.num_fc_nodes, name = 'fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        fc1_relu = tf.nn.relu(fc1_dropout)\n",
    "        # [batch_size*num_timesteps, vocab_size]\n",
    "        logits = tf.layers.dense(fc1_relu,\n",
    "                                 vocab_size,\n",
    "                                 name = 'logits')\n",
    "        \n",
    "    # Calculates loss.\n",
    "    with tf.variable_scope('loss'):\n",
    "        sentence_flatten = tf.reshape(sentence, [-1])\n",
    "        mask_flatten = tf.reshape(mask, [-1])\n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        \n",
    "        # [batch_size*num_timesteps,]\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits, labels = sentence_flatten)\n",
    "        \n",
    "        weighted_softmax_loss = tf.multiply(\n",
    "            softmax_loss, tf.cast(mask_flatten, tf.float32))\n",
    "        loss = tf.reduce_sum(weighted_softmax_loss) / tf.cast(mask_sum, tf.float32)\n",
    "        \n",
    "        prediction = tf.argmax(logits, 1, output_type = tf.int32)\n",
    "        correct_prediction = tf.equal(prediction,\n",
    "                                      sentence_flatten)\n",
    "        weighted_correct_prediction = tf.multiply(\n",
    "        tf.cast(correct_prediction, tf.float32), tf.cast(mask_flatten, tf.float32))\n",
    "        accuracy = tf.reduce_sum(weighted_correct_prediction) / tf.cast(mask_sum, tf.float32)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        \n",
    "    # Define train op.\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            logging.info('variable name: %s' % var.name)\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step = global_step)\n",
    "        \n",
    "    return ((img_feature, sentence, mask, keep_prob),\n",
    "            (loss, accuracy, train_op),\n",
    "             global_step)\n",
    "placeholders, metrics, global_step = get_train_model(\n",
    "    hps, vocab_size, img_feature_dim)\n",
    "img_feature, sentence, mask, keep_prob = placeholders\n",
    "loss, accuracy, train_op = metrics\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep = 10)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:   100, loss: 6.064, acc: 0.124\n",
      "INFO:tensorflow:Step:   200, loss: 5.518, acc: 0.165\n",
      "INFO:tensorflow:Step:   300, loss: 4.185, acc: 0.402\n",
      "INFO:tensorflow:Step:   400, loss: 3.955, acc: 0.399\n",
      "INFO:tensorflow:Step:   500, loss: 4.007, acc: 0.396\n",
      "INFO:tensorflow:Step:   600, loss: 3.749, acc: 0.421\n",
      "INFO:tensorflow:Step:   700, loss: 3.387, acc: 0.474\n",
      "INFO:tensorflow:Step:   800, loss: 2.980, acc: 0.535\n",
      "INFO:tensorflow:Step:   900, loss: 3.153, acc: 0.493\n",
      "INFO:tensorflow:Step:  1000, loss: 3.056, acc: 0.522\n",
      "INFO:tensorflow:Step:  1000, model saved\n",
      "INFO:tensorflow:Step:  1100, loss: 2.902, acc: 0.541\n",
      "INFO:tensorflow:Step:  1200, loss: 2.685, acc: 0.579\n",
      "INFO:tensorflow:Step:  1300, loss: 2.631, acc: 0.580\n",
      "INFO:tensorflow:Step:  1400, loss: 2.497, acc: 0.592\n",
      "INFO:tensorflow:Step:  1500, loss: 2.130, acc: 0.653\n",
      "INFO:tensorflow:Step:  1600, loss: 2.344, acc: 0.609\n",
      "INFO:tensorflow:Step:  1700, loss: 2.199, acc: 0.639\n",
      "INFO:tensorflow:Step:  1800, loss: 2.286, acc: 0.632\n",
      "INFO:tensorflow:Step:  1900, loss: 2.150, acc: 0.641\n",
      "INFO:tensorflow:Step:  2000, loss: 2.314, acc: 0.610\n",
      "INFO:tensorflow:Step:  2000, model saved\n",
      "INFO:tensorflow:Step:  2100, loss: 2.337, acc: 0.617\n",
      "INFO:tensorflow:Step:  2200, loss: 2.091, acc: 0.648\n",
      "INFO:tensorflow:Step:  2300, loss: 2.259, acc: 0.622\n",
      "INFO:tensorflow:Step:  2400, loss: 1.960, acc: 0.671\n",
      "INFO:tensorflow:Step:  2500, loss: 1.940, acc: 0.690\n",
      "INFO:tensorflow:Step:  2600, loss: 1.813, acc: 0.694\n",
      "INFO:tensorflow:Step:  2700, loss: 1.990, acc: 0.672\n",
      "INFO:tensorflow:Step:  2800, loss: 1.842, acc: 0.682\n",
      "INFO:tensorflow:Step:  2900, loss: 2.014, acc: 0.658\n",
      "INFO:tensorflow:Step:  3000, loss: 1.845, acc: 0.690\n",
      "INFO:tensorflow:Step:  3000, model saved\n",
      "INFO:tensorflow:Step:  3100, loss: 1.839, acc: 0.682\n",
      "INFO:tensorflow:Step:  3200, loss: 1.842, acc: 0.681\n",
      "INFO:tensorflow:Step:  3300, loss: 1.928, acc: 0.679\n",
      "INFO:tensorflow:Step:  3400, loss: 1.831, acc: 0.693\n",
      "INFO:tensorflow:Step:  3500, loss: 1.927, acc: 0.679\n",
      "INFO:tensorflow:Step:  3600, loss: 1.699, acc: 0.713\n",
      "INFO:tensorflow:Step:  3700, loss: 1.869, acc: 0.694\n",
      "INFO:tensorflow:Step:  3800, loss: 1.687, acc: 0.712\n",
      "INFO:tensorflow:Step:  3900, loss: 1.660, acc: 0.713\n",
      "INFO:tensorflow:Step:  4000, loss: 1.715, acc: 0.713\n",
      "INFO:tensorflow:Step:  4000, model saved\n",
      "INFO:tensorflow:Step:  4100, loss: 1.656, acc: 0.718\n",
      "INFO:tensorflow:Step:  4200, loss: 1.467, acc: 0.740\n",
      "INFO:tensorflow:Step:  4300, loss: 1.553, acc: 0.737\n",
      "INFO:tensorflow:Step:  4400, loss: 1.651, acc: 0.716\n",
      "INFO:tensorflow:Step:  4500, loss: 1.657, acc: 0.711\n",
      "INFO:tensorflow:Step:  4600, loss: 1.578, acc: 0.722\n",
      "INFO:tensorflow:Step:  4700, loss: 1.791, acc: 0.683\n",
      "INFO:tensorflow:Step:  4800, loss: 1.749, acc: 0.692\n",
      "INFO:tensorflow:Step:  4900, loss: 1.516, acc: 0.731\n",
      "INFO:tensorflow:Step:  5000, loss: 1.674, acc: 0.709\n",
      "INFO:tensorflow:Step:  5000, model saved\n",
      "INFO:tensorflow:Step:  5100, loss: 1.678, acc: 0.708\n",
      "INFO:tensorflow:Step:  5200, loss: 1.575, acc: 0.725\n",
      "INFO:tensorflow:Step:  5300, loss: 1.492, acc: 0.734\n",
      "INFO:tensorflow:Step:  5400, loss: 1.495, acc: 0.737\n",
      "INFO:tensorflow:Step:  5500, loss: 1.668, acc: 0.715\n",
      "INFO:tensorflow:Step:  5600, loss: 1.535, acc: 0.739\n",
      "INFO:tensorflow:Step:  5700, loss: 1.568, acc: 0.728\n",
      "INFO:tensorflow:Step:  5800, loss: 1.558, acc: 0.719\n",
      "INFO:tensorflow:Step:  5900, loss: 1.627, acc: 0.710\n",
      "INFO:tensorflow:Step:  6000, loss: 1.384, acc: 0.764\n",
      "INFO:tensorflow:Step:  6000, model saved\n",
      "INFO:tensorflow:Step:  6100, loss: 1.640, acc: 0.712\n",
      "INFO:tensorflow:Step:  6200, loss: 1.708, acc: 0.703\n",
      "INFO:tensorflow:Step:  6300, loss: 1.520, acc: 0.724\n",
      "INFO:tensorflow:Step:  6400, loss: 1.567, acc: 0.729\n",
      "INFO:tensorflow:Step:  6500, loss: 1.511, acc: 0.738\n",
      "INFO:tensorflow:Step:  6600, loss: 1.667, acc: 0.709\n",
      "INFO:tensorflow:Step:  6700, loss: 1.554, acc: 0.739\n",
      "INFO:tensorflow:Step:  6800, loss: 1.527, acc: 0.731\n",
      "INFO:tensorflow:Step:  6900, loss: 1.549, acc: 0.726\n",
      "INFO:tensorflow:Step:  7000, loss: 1.583, acc: 0.728\n",
      "INFO:tensorflow:Step:  7000, model saved\n",
      "INFO:tensorflow:Step:  7100, loss: 1.661, acc: 0.707\n",
      "INFO:tensorflow:Step:  7200, loss: 1.407, acc: 0.744\n",
      "INFO:tensorflow:Step:  7300, loss: 1.462, acc: 0.751\n",
      "INFO:tensorflow:Step:  7400, loss: 1.313, acc: 0.770\n",
      "INFO:tensorflow:Step:  7500, loss: 1.575, acc: 0.726\n",
      "INFO:tensorflow:Step:  7600, loss: 1.485, acc: 0.736\n",
      "INFO:tensorflow:Step:  7700, loss: 1.594, acc: 0.725\n",
      "INFO:tensorflow:Step:  7800, loss: 1.480, acc: 0.738\n",
      "INFO:tensorflow:Step:  7900, loss: 1.343, acc: 0.759\n",
      "INFO:tensorflow:Step:  8000, loss: 1.582, acc: 0.712\n",
      "INFO:tensorflow:Step:  8000, model saved\n",
      "INFO:tensorflow:Step:  8100, loss: 1.325, acc: 0.759\n",
      "INFO:tensorflow:Step:  8200, loss: 1.819, acc: 0.681\n",
      "INFO:tensorflow:Step:  8300, loss: 1.556, acc: 0.723\n",
      "INFO:tensorflow:Step:  8400, loss: 1.526, acc: 0.731\n",
      "INFO:tensorflow:Step:  8500, loss: 1.641, acc: 0.711\n",
      "INFO:tensorflow:Step:  8600, loss: 1.483, acc: 0.739\n",
      "INFO:tensorflow:Step:  8700, loss: 1.618, acc: 0.708\n",
      "INFO:tensorflow:Step:  8800, loss: 1.339, acc: 0.754\n",
      "INFO:tensorflow:Step:  8900, loss: 1.505, acc: 0.732\n",
      "INFO:tensorflow:Step:  9000, loss: 1.418, acc: 0.747\n",
      "INFO:tensorflow:Step:  9000, model saved\n",
      "INFO:tensorflow:Step:  9100, loss: 1.593, acc: 0.714\n",
      "INFO:tensorflow:Step:  9200, loss: 1.486, acc: 0.738\n",
      "INFO:tensorflow:Step:  9300, loss: 1.596, acc: 0.727\n",
      "INFO:tensorflow:Step:  9400, loss: 1.614, acc: 0.714\n",
      "INFO:tensorflow:Step:  9500, loss: 1.483, acc: 0.738\n",
      "INFO:tensorflow:Step:  9600, loss: 1.518, acc: 0.729\n",
      "INFO:tensorflow:Step:  9700, loss: 1.542, acc: 0.726\n",
      "INFO:tensorflow:Step:  9800, loss: 1.395, acc: 0.753\n",
      "INFO:tensorflow:Step:  9900, loss: 1.517, acc: 0.727\n",
      "INFO:tensorflow:Step: 10000, loss: 1.422, acc: 0.744\n",
      "INFO:tensorflow:Step: 10000, model saved\n"
     ]
    }
   ],
   "source": [
    "training_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "    for i in range(training_steps):\n",
    "        (batch_img_features,\n",
    "         batch_sentence_ids,\n",
    "         batch_weights, _)= caption_data.next_batch(hps.batch_size)\n",
    "        input_vals = (batch_img_features,\n",
    "                      batch_sentence_ids,\n",
    "                      batch_weights,\n",
    "                      hps.keep_prob)\n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "        should_log = (i + 1) % hps.log_frequent == 0\n",
    "        should_save = (i + 1) % hps.save_frequent == 0\n",
    "        \n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "            \n",
    "        outputs = sess.run(fetches, feed_dict = feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if should_log:\n",
    "            summary_str = outputs[-1]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info('Step: %5d, loss: %3.3f, acc: %3.3f'\n",
    "                         % (global_step_val, loss_val, accuracy_val))\n",
    "        if should_save:\n",
    "            model_save_file = os.path.join(output_dir, 'image_caption')\n",
    "            logging.info('Step: %5d, model saved' % global_step_val)\n",
    "            saver.save(sess,\n",
    "                       model_save_file,\n",
    "                      global_step = global_step_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
